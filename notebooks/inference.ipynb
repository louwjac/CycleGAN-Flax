{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHQucsi4B_kM"
      },
      "source": [
        "# CycleGAN in Jax + Flax inference demo\n",
        "\n",
        "This notebook demonstrates how to use pre-trained model weights to generate images with the CycleGAN model code published here: https://github.com/louwjac/CycleGAN-Flax\n",
        "<br/>Please make sure that you've selected a GPU runtime instance before proceeding. This notebook will not run on Colab TPU's.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2AotvHYB_ko"
      },
      "outputs": [],
      "source": [
        "#clone the cyclegan code repo\n",
        "!git clone https://github.com/louwjac/CycleGAN-Flax.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f92zOLIBB_kr"
      },
      "outputs": [],
      "source": [
        "#get the model weights from huggingface\n",
        "%cd CycleGAN-Flax\n",
        "!rm -r ./experiments\n",
        "!git clone https://huggingface.co/louwjac/CycleGAN-Flax.git experiments/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies\n",
        "!pip install -r notebooks/requirements.txt"
      ],
      "metadata": {
        "id": "hnAhBl9SKj1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSCdmUZxB_kt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ntpath\n",
        "\n",
        "import gin\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from flax.training import checkpoints\n",
        "\n",
        "from cyclegan import models, utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1_W7TGfB_kv"
      },
      "source": [
        "## A note on Gin\n",
        "\n",
        "Gin is a framework that can be used to configure input parameters in Python. It takes the place of the traditional config-dict, param, ini or yaml files that you may be used to. Please see the documentation included with the official repo (https://github.com/google/gin-config) if you are not already familiar with it. \n",
        "\n",
        "The rest of this demo will highlight all instances where Gin configured parameters are used. I recommend that you open one of the [config files]( https://github.com/louwjac/CycleGAN-Flax/tree/main/config) separately to get a sense of what has been configured. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vF-PDp-B_kx"
      },
      "outputs": [],
      "source": [
        "# Load the gin config file that was used to train the model\n",
        "# This makes it easy to play with a different model. Just change the config file here and re-run the code cells\n",
        "cfg_path = 'config/horse2zebra_original.gin' \n",
        "cfg = gin.parse_config_file(cfg_path)\n",
        "\n",
        "#get the name of the gin file\n",
        "cfg_filename = ntpath.basename(cfg_path).split(\".\")[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdmgS1CkB_kz"
      },
      "outputs": [],
      "source": [
        "work_dir = gin.query_parameter(\"%work_dir\") #read the value from line 6 of the .gin file e.g. \"work_dir = './experiments'\"\n",
        "work_dir = os.path.join(work_dir, cfg_filename)\n",
        "checkpoints_dir = os.path.join( work_dir, \"checkpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKzQ9bzmB_k1"
      },
      "source": [
        "# Load pre-trained weights\n",
        "\n",
        "Each CycleGAN model makes use of two generator networks; one to translate images from domain A to domain B, and another to do the reverse. For this demo, one network will turn horses into fake zebras and another will turn zebras into fake horses.\n",
        "\n",
        "So,\n",
        ">Network_G: Horse -> Fake Zebra<br>\n",
        ">Network_F:  Zebra -> Fake Horse\n",
        "\n",
        "This codebase combines both generator networks into a single \"CycleGenerator\" class in order to make the training code cleaner and the data structures easier to handle. As a result, the model weights for both networks are joined into a single data structure for the CycleGenerator. We will therefore need to extract the weights for each sub-network from the larger structure before we'll be able to do inference with them separately. This is fortunately very easy to do with Flax models. \n",
        "\n",
        "The next few cells will execute the necessary steps to load pre-trained weights. This will consist of:\n",
        "1) Create an instance of the model class.\n",
        "2) Initialize model weights for the model instance. This will be the same as the weights you would normally start with in a training session.\n",
        "3) Load the pre-trained model weights from disk. The initialized weights are passed in to the loader to serve as a template telling Flax how to load the stored weights. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKBus8m2B_k3"
      },
      "source": [
        "## Step 1: Create a model instance\n",
        "This is a case where gin is used to configure the model: ``` generator = models.CycleGenerator()```\n",
        "\n",
        "Notice that CycleGenerator() is called without arguments. Now take a look at lines 69-73 of [horse2zebra_original.gin](https://github.com/louwjac/CycleGAN-Flax/blob/main/config/horse2zebra_original.gin) :\n",
        "\n",
        "```\n",
        "  69 # Parameters for CycleGenerator:\n",
        "  70 # ==============================================================================\n",
        "  71 models.ResnetGenerator.residuals = 9\n",
        "  72 models.ResnetGenerator.features = 64\n",
        "  73 models.CycleGenerator.base_model = @models.ResnetGenerator\n",
        "```\n",
        "\n",
        "CycleGenerator takes an argument named \"base_model\" that is used as the model class for each of the sub-networks G and F. In this case, that \"base_model\" parameter has been configured to use an instance of a ResnetGenerator class, which itself has also been configured. Both classes are defined in cyclegan/models.py, hence the prefix \"models.CycleGenerator\" and \"models.ResnetGenerator\". Gin will bind the arguments that were configured in horse2zebra_original.gin automatically when we call models.CycleGenerator() without those arguments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djuz34-iB_k5"
      },
      "outputs": [],
      "source": [
        "#initialize a cyclegan generator\n",
        "generator = models.CycleGenerator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXH5WYcsB_k6"
      },
      "source": [
        "## Step 2: Initialize model weights\n",
        "This will create a data structure that will contain initialized model weights similar to what you will use when you start a training session. This is only needed here because it will inform Flax how the trained model weights should be structured when they are loaded. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbzAeaPdB_k7"
      },
      "outputs": [],
      "source": [
        "kg = utils.KeyGen()\n",
        "rngs_gen = {'params': kg()}\n",
        "sample_batch = (jnp.ones(shape=(1,256,256,3)),)*2 # this is a dummy batch of input images that will inform Flax of the input shapes\n",
        "vars_gen = generator.init(rngs_gen, sample_batch)\n",
        "\n",
        "# Flax model parameters can include non-trainable states (such as batch statistics)\n",
        "# but CycleGan does not use any such components. So, we can discard them here. \n",
        "# \"params_gen\" will include only the trainable weights of both networks G and F\n",
        "states_gen, params_gen = vars_gen.pop('params') \n",
        "del vars_gen, states_gen\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run the next line to inspect the shape of the weights structure in case you are curious\n",
        "utils.print_shapes(params_gen)"
      ],
      "metadata": {
        "id": "73ebHmrqNFXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg1niV_ZB_k8"
      },
      "source": [
        "## Step 3: Load the saved model weights\n",
        "The saved models were stored using the checkpoints utility that is included in Flax. That utility is in the process of being [deprecated](https://github.com/google/flax/discussions/2720) in favor of a library named [Orbax](https://github.com/google/orbax). Keep that in mind when you get to a point where you need to checkpoint your own models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EautZASbB_k8"
      },
      "outputs": [],
      "source": [
        "#restore generator model weights from a pre-trained checkpoint\n",
        "params_gen = checkpoints.restore_checkpoint(\n",
        "    ckpt_dir=checkpoints_dir,\n",
        "    target=params_gen,\n",
        "    prefix='params_'\n",
        ")\n",
        "\n",
        "#extract the weights for the two sub-networks from the generator\n",
        "params_gen, params_g = params_gen.pop('net_g')\n",
        "params_gen, params_f = params_gen.pop('net_f')\n",
        "\n",
        "#params_gen is now empty\n",
        "utils.print_shapes(params_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhwQgHViB_k9"
      },
      "source": [
        "# Make the inference function\n",
        "The following steps are required to do inference:\n",
        "\n",
        "1. Get an input image\n",
        "2. Resize it so that it is roughly in the same scale as the images that were used to train the model\n",
        "3. Convert the image to an array \n",
        "4. Convert the RGB pixel values from the integer range [0,255] to the floating-point range [-1.0, 1.0]\n",
        "5. Feed the image array into the generator model and get back a translated array with the same shape\n",
        "6. Convert the output pixel values from the floating-point range of [-1.0, 1.0] back to the integer range [0, 255]\n",
        "7. Change the array back into an image object\n",
        "\n",
        "The next cell will produce a single function for each of the sub-networks G and F that can complete steps 2 through 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrj2rLLBB_k-"
      },
      "outputs": [],
      "source": [
        "# make the inference function\n",
        "\n",
        "def resize_img(img: Image):  \n",
        "  # this takes care of step 2\n",
        "  height, width = img.size\n",
        "  scale_in = min(1,256./max(1.,min(height,width)))\n",
        "  height= int(scale_in*height)\n",
        "  width= int(scale_in*width)\n",
        "  img = img.resize(((height, width)))\n",
        "  return img\n",
        "\n",
        "def generate_fn(params):\n",
        "  base_model = models.ResnetGenerator() # don't need to pass any parameters here because it is taken care of in the gin config file.\n",
        "\n",
        "  @jax.jit\n",
        "  def gen_fake(inputs):\n",
        "    # this is step 5\n",
        "    fake_img = base_model.apply({'params':params}, inputs)\n",
        "    return fake_img\n",
        "\n",
        "  def fn(img_in):\n",
        "    if img_in is None:\n",
        "      return img_in\n",
        "\n",
        "    # step 2: resize\n",
        "    img_out = resize_img(img_in) \n",
        "\n",
        "    # step 3: convert to array\n",
        "    img_out = np.asarray(img_out, dtype=np.uint8)\n",
        "\n",
        "    # step 4: convert values from RGB\n",
        "    # this also changes the numpy array into a jax devicearray and loads it into GPU memory\n",
        "    img_out = (jnp.asarray(img_out) +127.5 ) - 1.0\n",
        "    img_out = jnp.expand_dims(img_out,0) #the model expects a batch dimention \n",
        "\n",
        "    # step 5: the exciting part!\n",
        "    img_out = gen_fake(img_out)\n",
        "\n",
        "    # step 6: convert values back to RGB\n",
        "    img_out = jnp.uint8((img_out[0] +1.0)*127.5)\n",
        "\n",
        "    # step 7: make an image object with the output\n",
        "    img_out = Image.fromarray(np.asarray(img_out),mode='RGB')\n",
        "\n",
        "    return img_out\n",
        "    \n",
        "  return fn\n",
        "\n",
        "\n",
        "# notice that \"generate_fn\" returns a function\n",
        "generate_g = generate_fn(params_g) \n",
        "generate_f = generate_fn(params_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdnclG-IB_k_"
      },
      "source": [
        "## Test the inference functions\n",
        "The CycleGAN repository contains a few sample images that have been selected because they work very well with the pre-trained models. Here we will open two of sample images and verify that the inference functions can generate fake horses and zebras. You should consider these results to be the best-case outputs for the models. Most other images will not produce great results from your point of view. Part of the reason for this is that the criteria you use to judge what a great output looks like are very different from the objectives the models were trained with. \n",
        "\n",
        "The training objective of horse2zebra model was not to translate horses to zebras. Instead, the objective was to generate images from the **horse image \"domain\"** to the **zebra image \"domain\"** and vice versa. The key difference is that an image domain is everything in the training sets, not just the horses and zebras. In other words, the models will make tradeoffs to produce good translations with respect to features you won't even notice instead of just focusing on making good fake horses and zebras. You, on the other hand, will likely judge an output to be bad if the fake horse or fake zebra doesn't look believable even if the model did a fantastic job of translating the grass, trees, sky and water. You will also not notice when those other items look bad but the horse or zebra looks good. \n",
        "\n",
        "It will take trial and error to find input images that produce outputs that look great through your eyes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRsq07m-B_lA"
      },
      "source": [
        "### Test horse2zebra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOy6uru4B_lB"
      },
      "outputs": [],
      "source": [
        "# Load a sample horse image\n",
        "horse_img = Image.open('./images/aleksei-zaitcev-ZZ68lVMON7g-unsplash.jpg')  #credit Aleksei Zaitcev (https://unsplash.com/@laowai66)\n",
        "# uncomment any lines below to try a different image\n",
        "# horse_img = Image.open('./images/brendon-van-zyl-PsdLrhj18bg-unsplash.jpg') #credit Brendon van Zyl (https://unsplash.com/@brendonvzyl)\n",
        "# horse_img = Image.open('./images/immo-wegmann-HT07wMriR1U-unsplash.jpg') #credit Immo Wegmann (https://unsplash.com/@macroman)\n",
        "\n",
        "#translate it to a zebra\n",
        "fake_zebra_img = generate_g(horse_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac8soVTTB_lC"
      },
      "outputs": [],
      "source": [
        "#view the input horse image in the same size as it was fed into the model\n",
        "resize_img(horse_img).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvs_7IT1B_lC"
      },
      "outputs": [],
      "source": [
        "#view the output\n",
        "fake_zebra_img.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHkKb-g5B_lD"
      },
      "source": [
        "### Test zebra2horse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssnPR4qwB_lE"
      },
      "outputs": [],
      "source": [
        "#Load a sample horse image\n",
        "zebra_img = Image.open('./images/henning-borgersen-SarK3PsCKnk-unsplash.jpg')  #credit Henning Borgersen (https://unsplash.com/@hebo79)\n",
        "#uncomment any lines below to try a different image\n",
        "# zebra_img = Image.open('./images/matteo-di-iorio-v-9hnUGyuOU-unsplash.jpg') #credit Matteo Di Iorio (https://unsplash.com/@shot_by_teo)\n",
        "# zebra_img = Image.open('./images/ray-rui-TwG9EZ28nms-unsplash.jpg') #credit Ray Rui (https://unsplash.com/@ray30)\n",
        "# zebra_img = Image.open('./images/ron-dauphin-k-8-eX4Y3no-unsplash.jpg') #credit https://unsplash.com/@rondomondo\n",
        "# zebra_img = Image.open('./images/sandra-gabriel-9yYrpdGu8g0-unsplash.jpg') #credit Sandra Gabriel (https://unsplash.com/@sandragabriel)\n",
        "# zebra_img = Image.open('./images/wolfgang-hasselmann-3UMTQDO5TkE-unsplash.jpg') #credit Wolfgang Hasselmann (https://unsplash.com/@wolfgang_hasselmann)\n",
        "\n",
        "#translate it to a zebra\n",
        "fake_horse_img = generate_f(zebra_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3ilVEfhB_lF"
      },
      "outputs": [],
      "source": [
        "#view the input zebra image in the same size as it was fed into the model\n",
        "resize_img(zebra_img).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk3dzmBWB_lF"
      },
      "outputs": [],
      "source": [
        "#view the output\n",
        "fake_horse_img.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbfVhAACB_lG"
      },
      "source": [
        "## Make a web app with Gradio\n",
        "[Gradio](https://gradio.app/) is a very easy-to-use Python library that enables you to quickly make web applications for Python functions. It is heavily used to show demos of deep learning models on [Huggingface Spaces](https://huggingface.co/spaces). One very nice feature that we will exploit here is that the input image in a Gradio app allows you to submit images to a model using a drag and drop interface. \n",
        "\n",
        "After you run the cell below, you will be able to try new images without changing any code. Simply open a separate browser window and do a Google image search for horses and zebras. Then drag and drop some of the results into the input boxes of the app to see what the models produce. Keep in mind that this app will not support all image formats that some websites will use. So, you may get errors on certain images. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYAEbsPoB_lG"
      },
      "outputs": [],
      "source": [
        "# Feel free to ignore the next line if you are not familiar with web development. \n",
        "# Css is used to style web pages. The next line is included because it helps to ensure that the input and output images \n",
        "# are displayed at roughly the same size in the app . \n",
        "css = \"\"\"\n",
        "    * {\n",
        "        margin:auto;\n",
        "    }\n",
        "    div.contain {\n",
        "        display:flex;\n",
        "        justify-content:center;\n",
        "    }\n",
        "\n",
        "    #in_img_a, \n",
        "    #in_img_b,\n",
        "    #out_img_a,\n",
        "    #out_img_b {\n",
        "        max-width: 400px;\n",
        "        max-height: 400px;\n",
        "        min-width: 256px\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "# The next few lines use the Gradio api to make a web app. Please see Gradio's documentation for more information. \n",
        "with gr.Blocks(theme=gr.themes.Glass() ,css=css) as demo:\n",
        "    gr.Markdown('# CycleGAN in Jax + Flax Demo')\n",
        "    gr.Markdown('### Do a Google image search for horses OR zebras (not together) in a separate window,<br/> then try dragging and dropping the images into the input boxes below!')\n",
        "    \n",
        "    with gr.Box():\n",
        "        atob = gr.Markdown(\"## Horse to Zebra\") \n",
        "        with gr.Row():       \n",
        "            inp = gr.Image(type=\"pil\",label=\"Input\", elem_id=\"in_img_a\")\n",
        "            out = gr.Image(type=\"pil\", label=\"Output\", interactive=False, elem_id=\"out_img_a\")                    \n",
        "            inp.change(lambda img:generate_g(img),  inputs=inp, outputs=out)\n",
        "        \n",
        "    with gr.Box():    \n",
        "        btoa = gr.Markdown(\"## Zebra to Horse\")\n",
        "        with gr.Row():\n",
        "            inp = gr.Image(type=\"pil\",label=\"Input\", elem_id=\"in_img_b\")\n",
        "            out = gr.Image(type=\"pil\", label=\"Output\", interactive=False, elem_id=\"out_img_b\")\n",
        "            inp.change(lambda img: generate_f(img),  inputs=inp, outputs=out)\n",
        "\n",
        "\n",
        "#Launch the demo! You should see the app in the output below after you run this cell\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}